# Chapter 6 : 메모리와 캐시 메모리

## 6.1. RAM의 특징과 종류

### 6.1.1. RAM의 특징
- 휘발성 저장 장치(Volatile Memory)
  - 전원을 끄면 저장된 내용이 사라지는 저장 장치다.
  - 대표적으로 RAM 이 있다.
- 비휘발성 저장 장치(Non-Volatile Memory) : 전원이 꺼져도 내용이 유지되는 저장 장치
  - 대표적으로 HDD, SSD, CD-ROM, USB 등이 있다. 보조 기억장치라고도 한다.
- 일반적으로 보조 기억장치에는 **보관할 대상** 을, RAM 에는 **실행할 대상** 을 저장한다.
- CPU가 프로그램을 실행할 때 보조 기억장치에 저장된 데이터를 가져와, RAM에 올리게 된다.
  - 이렇게 RAM으로 복사된 프로그램을 **프로세스(Process)** 라고 한다.

### 6.1.2. RAM의 용량과 성능
- RAM의 용량이 적으면, 프로그램을 보조 기억장치에서 빈번히 불러와야 한다.
- RAM이 가득 차면, 운영체제는 보조 기억장치로 일부 데이터를 옮겨 공간을 확보하고 필요한 프로그램이나 데이터를 RAM으로 불러온다.
- 주요 메모리 관리 기법으로 **페이징(Paging)** 과 **스와핑(Swapping)** 이 있다.
  - 페이징은 RAM을 '페이지'로 나누고, RAM이 가득 차면 가장 덜 사용되는 페이지를 보조 기억장치로 임시 저장(Swap Out)하거나 필요한 데이터나 명령어가 있을 때 다시 RAM으로 불러온다(Swap In).
  - 스와핑은 새 프로그램을 RAM에 로드하기 위해 이전 프로세스를 통째로 보조 기억장치로 옮기는 기법이다.

### 6.1.3. RAM의 종류

#### DRAM(Dynamic RAM)

![DRAM](https://mblogthumb-phinf.pstatic.net/MjAyMDEwMDRfOTQg/MDAxNjAxNzQyMDQ2NTI3.MDmZx1GtR8U-djNbgSjp7OO4XweD6FjwgDA2SBHOfC8g.V5X7wBxzXFOGfOEUTF23FjDQLfhS0uUTjxmXNnLTavog.JPEG.ycpiglet/254CD53958DB50692D.jpg?type=w800)

- 저장된 데이터가 시간이 지남에 따라 사라질 수 있으므로, 일정 주기로 데이터를 갱신해야 하는 '데이터 리프레시'가 필요하다.
- 일반적으로 사용되는 PC의 주 메모리이다.
- 낮은 소비 전력, 경제적인 비용, 그리고 높은 집적도로 대용량 메모리 설계가 용이하다.
  - DRAM은 주로 셀, 배선, 그리고 주변 회로로 구성되어 있다. 특히 각 셀은 하나의 트랜지스터(Transistor)와 하나의 커패시터(Capacitor)로 이뤄져 있어, 이 간단한 구조가 높은 집적도를 가능하게 한다.
    - 셀 : 메모리 칩 내에서 데이터를 저장하는 기본 단위이며, 각 메모리 셀은 특정한 데이터 비트 수를 저장한다.
    - 트랜지스터 : 전자 회로에서 신호를 증폭하거나 스위칭 역할을 하는 반도체 소자이다.
    - 커패시터 : 전하를 저장하는 반도체 소자이다. 
  - 고급 기술과 최적화가 다양하게 적용될 수 있다.
    > 예시 : 3D 스택 메모리나 채널 구조 변경 등이 있으며, AMD Radeon with HBM2에서는 3D 스택 메모리를 사용한다.
  - 고성능이나 저전력 등의 다양한 용도에 맞게 변형이 되기도 한다.
    > 예시 : Samsung LPDDR5는 저전력 목적으로, SK Hynix GDDR6는 고성능 그래픽 카드에 적합하게 설계되었다.
- 비동기적으로 동작하며 CPU가 메모리에 접근할 때마다 데이터 전송을 위한 타이밍을 조절해야 한다.
  - CPU와 DRAM 사이의 동작이 독립적이기 때문에, 데이터 요청이 들어왔을 때 DRAM은 응답하기 때문이다.
  
#### SRAM(Static RAM)
- Flip-Flop 회로를 사용하여 데이터가 안정적으로 유지되므로 리프레시가 필요하지 않다.
  - Flip-Flop 회로는 디지털 회로의 일종으로 설정(Set) 신호나 재설정(Reset) 신호가 들어오기 전까지 현재 상태를 유지하는 회로이다.
  - 이러한 특성 덕분에 데이터를 "기억"하게 되며, 따라서 지속적인 전원 공급만 있으면 현재 상태를 유지할 수 있다.
- DRAM 보다 집적도가 낮고, 소비 전력도 크고 가격도 더 비싸다.
  - SRAM 셀은 대체로 6개의 트랜지스터로 구성되어 있다. 이러한 복잡한 구조는 공간을 더 차지하게 하고, 집적도가 DRAM보다 낮다.
    > 예시 : Intel의 Stratix FPGA는 내장 SRAM을 가지고 있으며, 이 SRAM은 6개의 트랜지스터 구조를 사용한다.
- Flip-Flop 회로의 특성상 데이터를 바로 읽고 쓸 수 있어 접근 시간이 빠르다. 따라서 높은 속도의 데이터 접근이 필요한 캐시 메모리에서 주로 사용된다.

#### SDRAM(Synchronous Dynamic RAM)
- DRAM 기술을 기반으로 하되, 클럭 신호에 동기화되어 동작한다.
  - 메모리 컨트롤러와 CPU가 동일한 클럭을 공유하거나 타이밍이 맞춰져 있기 때문에 메모리 접근 타이밍이 일정하고 예측 가능하다.
  - 따라서 별도의 타이밍 조절이 필요하지 않다.
  > 예시 : Samsung의 DDR3 SDRAM은 클럭 동기화 기술을 사용하여 더 빠른 데이터 전송을 가능하게 한다.

#### DDR SDRAM(Double Data Rate SDRAM)

![클럭의 엣지](https://upload.wikimedia.org/wikipedia/commons/0/0e/Clock_signal.png)

- DDR이라는 이름은 한 클럭 주기에 두 번의 데이터 전송이 가능하기 때문에 붙여졌다.
- 높은 성능 요구에 효율적으로 대응하기 때문에 현재 가장 널리 사용된다.
- 클럭의 상승 엣지와 하강 엣지에서 데이터를 전송하여 대역폭을 증가시킨다.
  - 클럭은 일정한 주기로 상승과 하강을 반복하는 신호이다.
  - 상승 엣지는 신호가 낮은 수준에서 높은 수준으로 변경되는 시점이고, 하강 신호는 높은 수준에서 낮은 수준으로 변경되는 시점이다.
  - 이러한 방식으로 한 클럭 주기 내에서 두 번의 데이터 전송이 가능해진다.
  - 대역폭은 데이터 통신이나 신호 처리에서 얼마나 많은 데이터가 단위 시간 동안 전송될 수 있는지 나타내는 척도이다.

## 6.2. 메모리의 주소 공간
- 데이터들은 메모리의 주소라는 체계속에 저장된다.
- 메모리의 주소에는 **물리 주소**와 **논리 주소**가 있다.

### 6.2.1. 주소 바인딩(address binding)
- 보조 기억 장치에 저장된 프로그램이 주 기억 장치에 로드되어 프로세스가 될 때, 메모리의 주소를 매핑하는 작업이다.
- 주소 바인딩은 시점에 따라 세 가지로 나뉜다.
  - 컴파일 타임 바인딩
  - 로드 타임 바인딩
  - 런 타임 바인딩

#### 컴파일 바인딩
- 컴파일 시점에 미리 정의된 주소로 매핑한다.
- 미리 정의된 주소로 인해 메모리 충돌이 발생할 경우 프로그램을 수정하고 다시 컴파일해야 한다.
- 논리 주소와 물리 주소가 같다.

#### 로드 타임 바인딩
- 로드 시점에 메모리 주소를 결정하여 컴파일 바인딩에서 발생할 수 있는 충돌을 피한다.
  - 프로세스의 크기가 너무 커서 메모리를 침범할 위험이 있다.
- 주소 재배치 작업 때문에 로딩 시간이 길어질 수 있다.
- 논리 주소와 물리 주소는 다르지만, 주소가 로드 이후에는 변경되지 않는다.

#### 런 타임 바인딩
- 코드가 실행되는 시점에 물리 주소를 결정한다.
- 빈번한 주소 변환이 필요하여, 이 변환 작업은 MMU(Memory Management Unit)라는 하드웨어가 수행한다.
- 현대 시스템에서는 대부분 런 타임 바인딩을 사용한다.
- 논리 주소와 물리 주소는 다르며, 실행 중에도 변경될 수 있다.

### 6.2.2. 물리 주소와 논리 주소
- 새롭게 실행되는 프로그램은 메모리에 적재되고 실행이 종료된 프로그램은 메모리에서 해제된다.
- 같은 프로그램을 다시 실행해도 적재되는 주소가 다를 수 있다.
- 물리 주소(Physical Address)와 논리 주소(Logical Address)
  - 물리 주소는 정보가 실제로 저장된 하드웨어상의 주소를 의미한다.
  - 논리 주소는 각 실행 중인 프로그램에 할당된 메인 메모리의 주소를 가리킨다.
- CPU가 메모리와 상호작용하려면 논리 주소와 물리 주소 간의 변환이 이루어져야 한다.
  - 이 변환은 CPU와 주소 버스 사이에 위치한 **메모리 관리 장치** 라는 하드웨어에 의해 수행된다.

#### 메모리 관리 장치(MMU; Memory Management Unit)
- CPU가 발생시킨 논리 주소를 물리 주소로 변환하는 역할을 한다.
  - 이 변환은 다양한 메커니즘을 통해 이루어질 수 있으며, 베이스 레지스터나 페이지 테이블 등이 사용될 수 있다.
    - 베이스 레지스터는 프로그램의 첫 물리 주소가 저장되며, 이는 단순한 주소 변환 방식에서 사용된다.
    - 페이지 테이블은 가상 주소를 물리 주소로 매핑하는 정보를 저장한 테이블이다. 이는 복잡한 메모리 관리 방식, 예를 들어 가상 메모리,에서 사용된다.

> 예시
> - 현재 베이스 레지스터에 15000이 저장되어 있고 CPU가 발생시킨 논리 주소가 100번지인 경우가 있다.
> - 이 논리 주소는 물리 주소 15100 으로 변환된다.

- MMU는 다양한 메모리 관리 기능을 수행한다.
  - 페이지 테이블을 사용하여 가상 메모리를 물리 메모리에 매핑한다.
    - 물리 메모리는 RAM을 지칭하기도 하지만, 실제 데이터가 저장되는 RAM의 부분을 가리키는 경우도 있다.
    - 가상 메모리는 물리 메모리보다 큰 메모리 공간을 프로그램에 제공하기 위한 디스크 공간을 일부 사용하는 기술이다.
  - 메모리 보호 기능을 통해 유효하지 않은 메모리 접근을 차단할 수 있다.

#### 메모리 보호 기법
- 논리 주소 범위를 벗어나는 명령어 실행을 방지하고, 실행 중인 프로그램이 다른 프로그램의 영항을 받지 않도록 보호하는 역할을 한다.
- 이러한 역할을 **한계 레지스터(Limit Register)** 가 수행한다.
  - 베이스 레지스터가 프로그램의 첫 물리 주소를 저장한다면, 한계 레지스터는 접근할 수 있는 논리 주소의 최대 범위를 저장한다.
- CPU는 메모리에 접근하기 전에 접근하려는 논리 주소가 한계 레지스터에 설정된 값보다 작은지 검사한다.
  - 범위 밖의 주소에 접근을 시도한다면 인터럽트(트랩)을 발생시켜 실행을 중단시킨다.
    - 시스템에 알림을 보내어 프로그램 실행을 중지하고, 대개는 오류 메시지를 출력하거나 프로그램을 종료시킨다.

## 6.3. 캐시 메모리
- CPU가 메모리로부터 데이터를 읽거나 쓰는 동안 대기해야 하므로, 그 성능은 메모리 접근 속도에 의해 제한될 수 있다.
- 이런 병목 현상을 해결하기 위해 도입된 중간 저장소가 캐시 메모리다.

### 6.3.1. 캐시 메모리의 등장 배경

#### 병목 현상의 발생
- 초기 에니악(ENIAC) 같은 기계에서는 연산을 변경하기 위해 모든 전선 및 회로들을 바꿔야 하는 Fixed Program Computers 방식을 사용했다.
- 이런 비효율성은 프로그램을 데이터와 함께 내부에 저장하는 Stored Program Computer 방식인 [폰 노이만 구조](https://en.wikipedia.org/wiki/Von_Neumann_architecture)가 도입되어 해결되었다.
- 그러나 폰 노이만 구조가 도입되고 난 후에도, CPU의 연산 속도와 메모리의 읽기/쓰기 속도 사이에는 여전히 큰 차이가 존재했다.
- 이로 인해 CPU가 자주 대기 상태에 빠지게 되어 병목 현상이 발생했다.

#### 초기 해결 노력과 한계
- 폰 노이만 구조가 널리 채택된 후에도 병목 현상을 해결하기 위한 여러 가지 노력이 있었다.
  - 메모리 자체의 속도를 높이려는 시도, 병렬 처리([하버드 구조](https://en.wikipedia.org/wiki/Harvard_architecture)), pipelining 등 다양한 아키텍처와 알고리즘의 도입이 있었다.
- 그러나 이러한 노력들로는 CPU와 메모리 사이의 병목 현상을 완전히 해결하기 어려웠다.
- 특히 데이터의 복잡성과 프로그램의 크기가 증가함에 따라, 병목 현상은 더욱 심화되었다.

### 6.3.2. 메모리 계층 구조(Memory Hierachy)

![메모리 계층 구조](https://rambinay.com.np/wp-content/uploads/2022/10/Screenshot-2022-10-24-083727.png)

- CPU가 데이터를 처리하기 전에 여러 메모리 계층을 거쳐 데이터를 로딩해야 한다.
- 일반적으로 보조 기억장치의 데이터는 주 기억장치를 거치고, 캐시를 거쳐 레지스터로 올라가게 되는데 반대의 경우도 마찬가지다.
- 빠른 접근 시간을 가진 메모리와 높은 저장 용량을 가진 메모리 사이에는 trade-off 관계가 있다.
  - 빠른 메모리는 일반적으로 높은 제조 비용을 요구하고, 큰 용량의 메모리는 데이터 검색이나 인덱싱이 복잡해져 접근 시간이 느려질 가능성이 높기 때문이다.
- CPU와 가까울수록 빠르고 비싸지며, 멀어질 수록 느리고 싸진다.

#### 메모리 계층 구조의 필요성
- 디코딩 속도
  - 메모리 계층 구조에서 디코딩이란, CPU가 메모리 내의 특정 위치에 저장된 데이터에 접근하기 위해 주소를 해석하는 과정을 디코딩이라고 한다.
  - 큰 메모리일수록 데이터 접근을 위한 디코딩 과정이 더 복잡하고 시간이 걸린다.
- 자주 쓰는 데이터 관리
  - 프로그램의 실행 중 모든 데이터는 고르게 접근되지 않고, 일부 데이터만 자주 사용된다.
  - 이러한 원리를 기반으로 캐시 메모리 등을 사용하여 자주 접근되는 데이터를 빠르게 처리할 수 있도록 설계되었다.
- 경제성
  - 빠른 메모리는 비싸므로 레지스터, 캐시, 주 기억장치, 보조 기억장치 등 다양한 계층의 메모리를 사용하여 비용과 성능을 최적화할 수 있다.

### 6.3.3. 캐시 메모리
- CPU와 메모리 사이에 위치하고, 레지스터 보다는 용량이 큰 메모리이다.
- SRAM을 활용하여 만들어진 메모리이다.
- 빈번한 메모리 접근 시간을 줄이기 위해 메모리에서 CPU가 사용할 외부 데이터를 미리 캐시 메모리에 할당한다.
- 코어와 가장 가까운 순서로 L1, L2, L3라고 부른다.
  - L1이 가장 빠르지만 용량이 작으며, 가격도 비싸다.
- L1은 명령어와 데이터 접근의 동시성을 향상시키기 위해 명령어 만을 저장하는 L1I와 데이터만을 저장하는 L1D로 나누기도 하는데, 이를 **분리형 캐시(Split Cache)** 라고 한다.

### 6.3.4. 참조 지역성 원리
- 캐시 메모리는 CPU가 사용할 것 같은 대상을 예측하여 저장한다.
- 캐시 히트(Cache Hit) : 캐시 메모리 내 데이터가 CPU에서 활용되는 경우
- 캐시 미스(Cache Miss) : 캐시에서 데이터를 찾으려 했지만 찾지 못하고, 메모리에서 직접 가져와야하는 경우
- 캐시 적중률(Cache Hit Ratio) : 캐시가 히트되는 비율
  - 계산식 : 캐시 히트 횟수 / (캐시 히트 횟수 + 미스 횟수)
- 캐시 메모리는 **참조 지역성의 원리(Locality of Reference. Principle of Locality)** 에 따라 메모리로부터 가져올 데이터를 저장한다.
  - 참조 지역성은 시간 지역성과 공간 지역성으로 나뉜다.

![시간 지역성과 공간 지역성](https://parksb.github.io/images/54877425-e73f7280-4e61-11e9-9526-d33a04c189f3.webp)

#### 시간 지역성(Temporal Locality)
- CPU가 한 번 접근한 메모리 위치에는 가까운 미래에 다시 접근할 확률이 높다.
- 이 원리에 따라, CPU가 최근에 사용한 데이터는 캐시에 유지되어 빠른 재접근이 가능하게 된다.
- 이를 통해 데이터나 명령어를 반복적으로 사용하는 루프 같은 코드에서 성능 향상을 기대할 수 있다.

#### 공간 지역성(Spatial Locality)
- CPU가 특정 메모리 위치에 접근할 경우, 그 주변의 메모리 위치에도 접근할 확률이 높다.
- 배열, 링크드 리스트와 같은 자료 구조에서 이 원리가 효과를 발휘한다.
- 공간 지역성을 활용하여, 메모리에서 연속된 블록을 미리 캐시로 불러옴으로써 성능을 향상시킬 수 있다.

### 6.3.5. 캐시 메모리 접근 방법
- 캐시는 해시 테이블 자료구조와 유사하게 Key를 통하여 공간에 접근할 수 있다.
  - 캐시의 공간은 블록단위(32Byte)로 구성되어 있다.
- CPU에서 접근하려는 메모리 주소가 캐시에 저장되어 있는지 확인하기 위해 다음과 같은 동작이 수행된다.
  > 블록 개수가 1024개이고 32비트 주소라고 가정
  > - [0, 4): block offset
  > - [4, 14): index bit
  > - [14, 31]: tag bit
  > 이후 아래의 과정을 통해 캐시에서 데이터를 처리한다.
  > 1. 인덱스 비트를 통해 태그 배열의 필드를 조회
  > 2. 태그 배열의 필드값이 태그 비트와 같고, 태그 배열의 유효비트가 참인지 확인
  > 3. 유효비트가 참이고 태그비트와도 같다면 캐시 히트 → 블록 오프셋을 활용하여(블록 단위 → 바이트 단위로 접근하게 함) 데이터 접근
  > 4. 유효비트가 거짓이라면 캐시 미스 → 주기억장치에서 조회 후 캐시 메모리에 저장
  > 5. 유효비트가 참인데, 태그비트는 같다면 교체 정책(FIFO, LRU 등)에 따라 어떤 데이터를 저장할지 결정

![캐시 메모리의 동작 예제 1](https://user-images.githubusercontent.com/71889359/263922767-2f4f6415-cd5c-40af-89a4-4cac7e13cfe7.png)

- 서로 다른 주소의 인덱스가 같고 두 주소 모두 자주 접근해야 한다면 해당 인덱스는 빈번하게 데이터 교체가 이루어질 수 있다.
- 이를 해결하기 위해 태그 배열과 데이터 배열을 여러 개 만들고, 병렬적으로 여러 배열을 조회하면 해결할 수 있다.

![캐시 메모리의 동작 예제 2](https://user-images.githubusercontent.com/71889359/263922791-e0556615-aa9c-4ab3-8386-8fd36fab8313.png)

#### 캐시 쓰기 정책
- 캐시 메모리는 읽기 작업 뿐만 아니라 쓰기 작업을 진행하는 경우도 있다.
- 쓰리 작업의 주소가 캐시 메모리에 저장되어있다면, 캐시 메모리의 데이터에 쓰여진다.
- 이 데이터는 주 기억 장치에 동기화가 이루어져야 하는데, 이를 위한 두 정책이 있다.
  - Write-through : 캐시에 데이터가 쓰여질 떄마다 주 기억 장치와 동기화한다.
    - 장점 : 캐시와 주 기억 장치의 데이터가 항상 일치하므로, 데이터 일관성이 높다.
    - 단점 : 매번 주 기억 장치에도 데이터를 쓰기 때문에, I/O 부하가 높아지고 성능이 저하될 수 있다.
  - Write-back : 블록에 수정이 있을 때 dirty bit를 1로 변경한다. 추후 블록이 변경될 때 주 기억 장치에 반영된다.
    - 장점 : 주 기억 장치로의 쓰기 연산이 줄어들어 성능이 향상된다.
    - 단점 : dirty bit를 관리해야 하며, 캐시와 주 기억 장치 간에 일시적인 데이터 불일치가 발생할 수 있다.

---

# 참고자료
- [[강해령의 하이엔드 테크] DRAM 특집: D램이라 쓰고, '다이나믹 듀오'라 읽는다](https://www.sedaily.com/NewsView/260ZMYQYEV)
- [Wikipedia - Page table](https://en.wikipedia.org/wiki/Page_table)
- [Wikipedia - Virtual Memory](https://en.wikipedia.org/wiki/Virtual_memory)
- [Wikipedia - 메모리 계층 구조](https://ko.wikipedia.org/wiki/%EB%A9%94%EB%AA%A8%EB%A6%AC_%EA%B3%84%EC%B8%B5_%EA%B5%AC%EC%A1%B0)
